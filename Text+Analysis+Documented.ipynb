{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Keywords identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries used for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import textblob\n",
    "import gensim\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim import corpora, models\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "import pyLDAvis.gensim #LDA visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function1 START\n",
    "## Read any EXCEL file, provided the path and sheet name\n",
    "def read_data_excel(path, sheet):\n",
    "    df=pd.read_excel(path, sheet_name=sheet)\n",
    "    print(\"****Details of the excel sheet below*****\")\n",
    "    print(\"Total Number of Rows in the excel dataset: \", len(df))\n",
    "    #print(\"Sample data in excel Dataset: \", df.head(6))\n",
    "    print(\"Column names in the excel Dataset: \", df.columns)\n",
    "    return df\n",
    "## Function1 END\n",
    "\n",
    "\n",
    "## Function1.1 START\n",
    "## Read any EXCEL file, provided the path and sheet name\n",
    "def read_data_csv(path):\n",
    "    df=pd.read_csv(path, error_bad_lines=False)\n",
    "    print(\"****Details of the CSV below*****\")\n",
    "    print(\"Total Number of Rows in the CSV dataset: \", len(df))\n",
    "    #print(\"Sample data in CSV Dataset: \", df.head(6))\n",
    "    print(\"Column names in the CSV Dataset: \", df.columns)\n",
    "    return df\n",
    "## Function1.1 END\n",
    "\n",
    "## Function2 START\n",
    "## Preprocessing of Data - conversion to lower case, \n",
    "## removing trailing and leading spaces, removing NULLs\n",
    "def preprocessing_data1(dataset_name):\n",
    "    dataset_name=dataset_name.apply(lambda x: x.astype(str).str.lower())\n",
    "    dataset_name=dataset_name.apply(lambda x: x.astype(str).str.strip())\n",
    "    dataset_name=dataset_name[~dataset_name['Comment_Made'].isnull()]\n",
    "    return dataset_name\n",
    "## Function2 END\n",
    "\n",
    "## Function3 START\n",
    "## Preprocessing of Data - removing stopwords and special characters\n",
    "def preprocessing_data2(dataset_name):\n",
    "    \n",
    "    #Fetching Stopwords from library nltk(natural language toolkit)\n",
    "    stopwords1=stopwords.words(\"english\")\n",
    "    #print(\"Type of the dataset stopword1 is: \",type(stopwords1))\n",
    "    \n",
    "    #Adding any additional stopwords, specific for the particular analysis\n",
    "    #stopwords1.append(['rocket','rock'])\n",
    "    stopwords_foc=['something','anything']\n",
    "    stopwords1.extend(stopwords_foc)\n",
    "    #print(stopwords1)\n",
    "    \n",
    "    #Removing stopwords\n",
    "    dataset_name['clean_comment']=\"\"\n",
    "    dataset_name['clean_comment_statement']=\"\"\n",
    "    for i, row in dataset_name.iterrows():\n",
    "        clean_comment=[]\n",
    "        words_comment=TextBlob(dataset_name['Comment_Made'][i])\n",
    "        words_comment1=words_comment.words\n",
    "        for words in words_comment1:\n",
    "            if (words not in stopwords1 and words not in string.punctuation):\n",
    "                clean_comment.append(words)\n",
    "            dataset_name['clean_comment'][i]=clean_comment\n",
    "        #print(dataset_name['clean_comment'][i])\n",
    "        dataset_name['clean_comment_statement'][i]=\" \".join(clean_comment)\n",
    "        #print(dataset_name['clean_comment_statement'][i])\n",
    "        \n",
    "    return dataset_name\n",
    "## Function3 END\n",
    "\n",
    "\n",
    "## Function3.1 START\n",
    "## Stemming of Data\n",
    "def preprocessing_data3(dataset_name):\n",
    "    #print(\"***************In the Stemming function******************\")\n",
    "    ps = PorterStemmer()\n",
    "    dataset_name['Stemming_words']=\"\"\n",
    "    dataset_name['Stemming_words_statements']=\"\"\n",
    "    #Stemming of text in user comments\n",
    "    for i,row in dataset_name.iterrows():\n",
    "        stemmed_words=[]\n",
    "        words_textblob=word_tokenize(dataset_name['clean_comment_statement'][i])\n",
    "        for words in words_textblob:\n",
    "            #print(words, \" : \", ps.stem(words))\n",
    "            stemmed_words.append(ps.stem(words))\n",
    "            dataset_name['Stemming_words'][i]=stemmed_words\n",
    "        dataset_name['Stemming_words_statements'][i]=\" \".join(stemmed_words)\n",
    "    return dataset_name\n",
    "## Function3.1 END\n",
    "\n",
    "\n",
    "## Function4 START\n",
    "## Sentiment Analysis usinh polarity\n",
    "def sentiment_analysis(dataset_name):\n",
    "    dataset_name['Polarity']=\"\"\n",
    "    dataset_name['Sentiment']=\"\"\n",
    "    dataset_name['Subjectivity']=\"\"\n",
    "    dataset_name['Polarity_cat']=\"\"\n",
    "    \n",
    "    for i,row in dataset_name.iterrows():\n",
    "        blob=TextBlob(dataset_name['clean_comment_statement'][i])\n",
    "        dataset_name['Polarity'][i]=blob.polarity\n",
    "        dataset_name['Sentiment'][i]=blob.sentiment\n",
    "        dataset_name['Subjectivity'][i]=blob.subjectivity\n",
    "        \n",
    "        #assigning categories for Polarity\n",
    "        if dataset_name['Polarity'][i] < 0:\n",
    "            dataset_name['Polarity_cat'][i]=\"Negative\"\n",
    "        if dataset_name['Polarity'][i] > 0:\n",
    "            dataset_name['Polarity_cat'][i]=\"Positive\"\n",
    "        if dataset_name['Polarity'][i] ==0:\n",
    "            dataset_name['Polarity_cat'][i]=\"Neutral\"\n",
    "    #print(dataset_name.head(10))\n",
    "    \n",
    "    print(\"Average Sentiment of the whole text: \", dataset_name['Polarity'].mean())\n",
    "    return dataset_name\n",
    "## Function4 END\n",
    "\n",
    "\n",
    "## Function5 START\n",
    "## Creating Word Cloud\n",
    "def word_cloud_fn(dataset_name):\n",
    "    #Word Cloud formation\n",
    "    txt=str(dataset_name['clean_comment_statement'])\n",
    "    txt1=str(dataset_name['Stemming_words_statements'])\n",
    "    wordcloud=WordCloud(width=800, height=800,\n",
    "                       background_color='white',\n",
    "                       min_font_size=10).generate(txt)\n",
    "    \n",
    "    #Plot the WordCloud image\n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.show()\n",
    "    return dataset_name\n",
    "## Function5 END\n",
    "\n",
    "\n",
    "## Function5.1 START\n",
    "## Feature Extraction\n",
    "def keyword_extract_fn(dataset_name):\n",
    "    #Converting the whole comments column into a text column\n",
    "    write_path_keyword=\"C:/Users/-----/User Comments/Keyword_found.csv\"\n",
    "    sentence_text=\" \".join(dataset_name['Stemming_words_statements'])\n",
    "    keyword_found=keywords(sentence_text, words=10).split('\\n')\n",
    "    #print(\"Type of Keyword ds: \", type(keyword_found))\n",
    "    #print(\"length of Keyword ds: \", len(keyword_found))\n",
    "    print(\"Keywords found in the whole text: \", keyword_found)\n",
    "    keyword_found_df=pd.DataFrame(keyword_found, columns=['Keywords'])\n",
    "    keyword_found_df.to_csv(write_path_keyword, index=False)\n",
    "        \n",
    "    return keyword_found \n",
    "## Function5.1 END\n",
    "\n",
    "## Function6 START\n",
    "## Group by blog name\n",
    "def group_by_fn(dataset_name):\n",
    "    #Group by command to group by Blog_Name\n",
    "    dataset_name1=dataset_name.groupby(\"Blog_Name\")[\"Comment_Made\"].transform(lambda x: \" \".join(x))\n",
    "    dataset_name1=dataset_name1.drop_duplicates\n",
    "    return dataset_name1\n",
    "## Function6 END\n",
    "\n",
    "## Function7 START\n",
    "## Writing the file into a CSV\n",
    "def write_csv_fn(dataset_name1, write_path):\n",
    "    #Write the file into a CSV\n",
    "    dataset_name1.to_csv(write_path)\n",
    "## Function7 END\n",
    "\n",
    "#set path and sheet name\n",
    "path=\"C:/Users/------/User_comments_on_leader_posts Aug10.csv\"\n",
    "sheet=\"Sheet1\"\n",
    "write_path=\"C:/Users/-----/User Comments/Sentiment_analysis2.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'main()' function and function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Details of the CSV below*****\n",
      "Total Number of Rows in the CSV dataset:  120\n",
      "Column names in the CSV Dataset:  Index(['Blog_Name', ' Commented_On_Date', 'Comment_Made', 'Commented_By_TM'], dtype='object')\n",
      "Average Sentiment of the whole text:  0.4188396473756321\n",
      "Keywords found in the whole text:  ['love', 'awesom', 'news', 'new', 'team', 'thank', 'great', 'detroit', 'solar', 'compani', 'help']\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    #Function1 CALL\n",
    "    #comments_data=read_data_excel(path, sheet)\n",
    "    \n",
    "    #Function1.1 CALL\n",
    "    comments_data=read_data_csv(path)\n",
    "    \n",
    "    #Function2 CALL\n",
    "    comments_data_processed=preprocessing_data1(comments_data)\n",
    "    \n",
    "    #Function3 CALL\n",
    "    #print(comments_data_processed.apply(lambda x: x.astype(str)))\n",
    "    comments_data_processed=preprocessing_data2(comments_data_processed)\n",
    "\n",
    "    #Function3.1 CALL\n",
    "    comments_data_processed=preprocessing_data3(comments_data_processed)\n",
    "    \n",
    "    #Function4 CALL\n",
    "    comments_data_processed=sentiment_analysis(comments_data_processed)\n",
    "    \n",
    "    #Function5 CALL\n",
    "    #word_cloud_fn(comments_data_processed)\n",
    "    \n",
    "    #Function5.1 CALL\n",
    "    keyword_found=keyword_extract_fn(comments_data_processed)\n",
    "    \n",
    "    #Function6 CALL\n",
    "    grouped_data_processed=group_by_fn(comments_data_processed)\n",
    "    \n",
    "    #Function7 CALL\n",
    "    write_csv_fn(comments_data_processed, write_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
